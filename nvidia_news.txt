Skip to content
Search for:
Home
AI
Data Center
Driving
Gaming
Pro Graphics
Robotics
Healthcare
Startups
AI Podcast
NVIDIA Life
NVIDIA Releases NIM Microservices to Safeguard Applications for Agentic AI
NVIDIA NeMo Guardrails includes new NVIDIA NIM microservices to enhance accuracy, security and control for enterprises building AI across industries.
January 16, 2025 by Kari Briski

 Share
  
AI agents are poised to transform productivity for the world’s billion knowledge workers with “knowledge robots” that can accomplish a variety of tasks. To develop AI agents, enterprises need to address critical concerns like trust, safety, security and compliance.

New NVIDIA NIM microservices for AI guardrails — part of the NVIDIA NeMo Guardrails collection of software tools — are portable, optimized inference microservices that help companies improve the safety, precision and scalability of their generative AI applications.

Central to the orchestration of the microservices is NeMo Guardrails, part of the NVIDIA NeMo platform for curating, customizing and guardrailing AI. NeMo Guardrails helps developers integrate and manage AI guardrails in large language model (LLM) applications. Industry leaders Amdocs, Cerence AI and Lowe’s are among those using NeMo Guardrails to safeguard AI applications.

Developers can use the NIM microservices to build more secure, trustworthy AI agents that provide safe, appropriate responses within context-specific guidelines and are bolstered against jailbreak attempts. Deployed in customer service across industries like automotive, finance, healthcare, manufacturing and retail, the agents can boost customer satisfaction and trust.

One of the new microservices, built for moderating content safety, was trained using the Aegis Content Safety Dataset — one of the highest-quality, human-annotated data sources in its category. Curated and owned by NVIDIA, the dataset is publicly available on Hugging Face and includes over 35,000 human-annotated data samples flagged for AI safety and jailbreak attempts to bypass system restrictions.

NVIDIA NeMo Guardrails Keeps AI Agents on Track
AI is rapidly boosting productivity for a broad range of business processes. In customer service, it’s helping resolve customer issues up to 40% faster. However, scaling AI for customer service and other AI agents requires secure models that prevent harmful or inappropriate outputs and ensure the AI application behaves within defined parameters.

NVIDIA has introduced three new NIM microservices for NeMo Guardrails that help AI agents operate at scale while maintaining controlled behavior:

Content safety NIM microservice that safeguards AI against generating biased or harmful outputs, ensuring responses align with ethical standards.
Topic control NIM microservice that keeps conversations focused on approved topics, avoiding digression or inappropriate content.
Jailbreak detection NIM microservice that adds protection against jailbreak attempts, helping maintain AI integrity in adversarial scenarios.
By applying multiple lightweight, specialized models as guardrails, developers can cover gaps that may occur when only more general global policies and protections exist — as a one-size-fits-all approach doesn’t properly secure and control complex agentic AI workflows.

Small language models, like those in the NeMo Guardrails collection, offer lower latency and are designed to run efficiently, even in resource-constrained or distributed environments. This makes them ideal for scaling AI applications in industries such as healthcare, automotive and manufacturing, in locations like hospitals or warehouses.

Industry Leaders and Partners Safeguard AI With NeMo Guardrails
NeMo Guardrails, available to the open-source community, helps developers orchestrate multiple AI software policies — called rails — to enhance LLM application security and control. It works with NVIDIA NIM microservices to offer a robust framework for building AI systems that can be deployed at scale without compromising on safety or performance.

Amdocs, a leading global provider of software and services to communications and media companies, is harnessing NeMo Guardrails to enhance AI-driven customer interactions by delivering safer, more accurate and contextually appropriate responses.

“Technologies like NeMo Guardrails are essential for safeguarding generative AI applications, helping make sure they operate securely and ethically,” said Anthony Goonetilleke, group president of technology and head of strategy at Amdocs. “By integrating NVIDIA NeMo Guardrails into our amAIz platform, we are enhancing the platform’s ‘Trusted AI’ capabilities to deliver agentic experiences that are safe, reliable and scalable. This empowers service providers to deploy AI solutions safely and with confidence, setting new standards for AI innovation and operational excellence.”

Cerence AI, a company specializing in AI solutions for the automotive industry, is using NVIDIA NeMo Guardrails to help ensure its in-car assistants deliver contextually appropriate, safe interactions powered by its CaLLM family of large and small language models.

“Cerence AI relies on high-performing, secure solutions from NVIDIA to power our in-car assistant technologies,” said Nils Schanz, executive vice president of product and technology at Cerence AI. “Using NeMo Guardrails helps us deliver trusted, context-aware solutions to our automaker customers and provide sensible, mindful and hallucination-free responses. In addition, NeMo Guardrails is customizable for our automaker customers and helps us filter harmful or unpleasant requests, securing our CaLLM family of language models from unintended or inappropriate content delivery to end users.”

Lowe’s, a leading home improvement retailer, is leveraging generative AI to build on the deep expertise of its store associates. By providing enhanced access to comprehensive product knowledge, these tools empower associates to answer customer questions, helping them find the right products to complete their projects and setting a new standard for retail innovation and customer satisfaction.

“We’re always looking for ways to help associates to above and beyond for our customers,” said Chandhu Nair, senior vice president of data, AI and innovation at Lowe’s. “With our recent deployments of NVIDIA NeMo Guardrails, we ensure AI-generated responses are safe, secure and reliable, enforcing conversational boundaries to deliver only relevant and appropriate content.”

To further accelerate AI safeguards adoption in AI application development and deployment in retail, NVIDIA recently announced at the NRF show that its NVIDIA AI Blueprint for retail shopping assistants incorporates NeMo Guardrails microservices for creating more reliable and controlled customer interactions during digital shopping experiences.

Consulting leaders Taskus, Tech Mahindra and Wipro are also integrating NeMo Guardrails into their solutions to provide their enterprise clients safer, more reliable and controlled generative AI applications.

NeMo Guardrails is open and extensible, offering integration with a robust ecosystem of leading AI safety model and guardrail providers, as well as AI observability and development tools. It supports integration with ActiveFence’s ActiveScore, which filters harmful or inappropriate content in conversational AI applications, and provides visibility, analytics and monitoring.

Hive, which provides its AI-generated content detection models for images, video and audio content as NIM microservices, can be easily integrated and orchestrated in AI applications using NeMo Guardrails.

The Fiddler AI Observability platform easily integrates with NeMo Guardrails to enhance AI guardrail monitoring capabilities. And Weights & Biases, an end-to-end AI developer platform, is expanding the capabilities of W&B Weave by adding integrations with NeMo Guardrails microservices. This enhancement builds on Weights & Biases’ existing portfolio of NIM integrations for optimized AI inferencing in production.

NeMo Guardrails Offers Open-Source Tools for AI Safety Testing
Developers ready to test the effectiveness of applying safeguard models and other rails can use NVIDIA Garak — an open-source toolkit for LLM and application vulnerability scanning developed by the NVIDIA Research team.

With Garak, developers can identify vulnerabilities in systems using LLMs by assessing them for issues such as data leaks, prompt injections, code hallucination and jailbreak scenarios. By generating test cases involving inappropriate or incorrect outputs, Garak helps developers detect and address potential weaknesses in AI models to enhance their robustness and safety.

Availability
NVIDIA NeMo Guardrails microservices, as well as NeMo Guardrails for rail orchestration and the NVIDIA Garak toolkit, are now available for developers and enterprises. Developers can get started building AI safeguards into AI agents for customer service using NeMo Guardrails with this tutorial.

See notice regarding software product information.

Categories: Generative AI
Tags: Artificial Intelligence | Cybersecurity | NVIDIA Blueprints | NVIDIA NeMo | NVIDIA NIM

Subscribe Widget
All NVIDIA News

How This NVIDIA Kaggle Grandmaster Merges Innovation and Play

Eternal Night Falls on GeForce NOW
Fantastic Four-ce Awakens: Season One of ‘Marvel Rivals’ Joins GeForce NOW


How AI Is Enhancing Surgical Safety and Education


NVIDIA GTC 2025: Quantum Day to Illuminate the Future of Quantum Computing


Healthcare Leaders, NVIDIA CEO Share AI Innovation Across the Industry

Post navigation
Unveiling a New Era of Local AI With NVIDIA NIM Microservices and AI Blueprints
New NIM microservices and AI Blueprints unlock generative AI on RTX AI PCs and workstation — plus, more announcements from CES recapped in this first installment of the RTX AI Garage series.
January 8, 2025 by Jesse Clayton

 Share
  
Over the past year, generative AI has transformed the way people live, work and play, enhancing everything from writing and content creation to gaming, learning and productivity. PC enthusiasts and developers are leading the charge in pushing the boundaries of this groundbreaking technology.

Countless times, industry-defining technological breakthroughs have been invented in one place — a garage. This week marks the start of the RTX AI Garage series, which will offer routine content for developers and enthusiasts looking to learn more about NVIDIA NIM microservices and AI Blueprints, and how to build AI agents, creative workflow, digital human, productivity apps and more on AI PCs. Welcome to the RTX AI Garage.

This first installment spotlights announcements made earlier this week at CES, including new AI foundation models available on NVIDIA RTX AI PCs that take digital humans, content creation, productivity and development to the next level.

These models — offered as NVIDIA NIM microservices — are powered by new GeForce RTX 50 Series GPUs. Built on the NVIDIA Blackwell architecture, RTX 50 Series GPUs deliver up to 3,352 trillion AI operations per second of performance, 32GB of VRAM and feature FP4 compute, doubling AI inference performance and enabling generative AI to run locally with a smaller memory footprint.

NVIDIA also introduced NVIDIA AI Blueprints — ready-to-use, preconfigured workflows, built on NIM microservices, for applications like digital humans and content creation.

NIM microservices and AI Blueprints empower enthusiasts and developers to build, iterate and deliver AI-powered experiences to the PC faster than ever. The result is a new wave of compelling, practical capabilities for PC users.

Fast-Track AI With NVIDIA NIM
There are two key challenges to bringing AI advancements to PCs. First, the pace of AI research is breakneck, with new models appearing daily on platforms like Hugging Face, which now hosts over a million models. As a result, breakthroughs quickly become outdated.

Second, adapting these models for PC use is a complex, resource-intensive process. Optimizing them for PC hardware, integrating them with AI software and connecting them to applications requires significant engineering effort.

NVIDIA NIM helps address these challenges by offering prepackaged, state-of-the-art AI models optimized for PCs. These NIM microservices span model domains, can be installed with a single click, feature application programming interfaces (APIs) for easy integration, and harness NVIDIA AI software and RTX GPUs for accelerated performance.

At CES, NVIDIA announced a pipeline of NIM microservices for RTX AI PCs, supporting use cases spanning large language models (LLMs), vision-language models, image generation, speech, retrieval-augmented generation (RAG), PDF extraction and computer vision.

The new Llama Nemotron family of open models provide high accuracy on a wide range of agentic tasks. The Llama Nemotron Nano model, which will be offered as a NIM microservice for RTX AI PCs and workstations, excels at agentic AI tasks like instruction following, function calling, chat, coding and math.

Soon, developers will be able to quickly download and run these microservices on Windows 11 PCs using Windows Subsystem for Linux (WSL).



To demonstrate how enthusiasts and developers can use NIM to build AI agents and assistants, NVIDIA previewed Project R2X, a vision-enabled PC avatar that can put information at a user’s fingertips, assist with desktop apps and video conference calls, read and summarize documents, and more. Sign up for Project R2X updates.

By using NIM microservices, AI enthusiasts can skip the complexities of model curation, optimization and backend integration and focus on creating and innovating with cutting-edge AI models.

What’s in an API?
An API is the way in which an application communicates with a software library. An API defines a set of “calls” that the application can make to the library and what the application can expect in return. Traditional AI APIs require a lot of setup and configuration, making AI capabilities harder to use and hampering innovation.

NIM microservices expose easy-to-use, intuitive APIs that an application can simply send requests to and get a response. In addition, they’re designed around the input and output media for different model types. For example, LLMs take text as input and produce text as output, image generators convert text to image, speech recognizers turn speech to text and so on.

The microservices are designed to integrate seamlessly with leading AI development and agent frameworks such as AI Toolkit for VSCode, AnythingLLM, ComfyUI, Flowise AI, LangChain, Langflow and LM Studio. Developers can easily download and deploy them from build.nvidia.com.

By bringing these APIs to RTX, NVIDIA NIM will accelerate AI innovation on PCs.

Enthusiasts are expected to be able to experience a range of NIM microservices using an upcoming release of the NVIDIA ChatRTX tech demo.

A Blueprint for Innovation
By using state-of-the-art models, prepackaged and optimized for PCs, developers and enthusiasts can quickly create AI-powered projects. Taking things a step further, they can combine multiple AI models and other functionality to build complex applications like digital humans, podcast generators and application assistants.

NVIDIA AI Blueprints, built on NIM microservices, are reference implementations for complex AI workflows. They help developers connect several components, including libraries, software development kits and AI models, together in a single application.



AI Blueprints include everything that a developer needs to build, run, customize and extend the reference workflow, which includes the reference application and source code, sample data, and documentation for customization and orchestration of the different components.

At CES, NVIDIA announced two AI Blueprints for RTX: one for PDF to podcast, which lets users generate a podcast from any PDF, and another for 3D-guided generative AI, which is based on FLUX.1 [dev] and expected be offered as a NIM microservice, offers artists greater control over text-based image generation.

With AI Blueprints, developers can quickly go from AI experimentation to AI development for cutting-edge workflows on RTX PCs and workstations.

Built for Generative AI
The new GeForce RTX 50 Series GPUs are purpose-built to tackle complex generative AI challenges, featuring fifth-generation Tensor Cores with FP4 support, faster G7 memory and an AI-management processor for efficient multitasking between AI and creative workflows.

The GeForce RTX 50 Series adds FP4 support to help bring better performance and more models to PCs. FP4 is a lower quantization method, similar to file compression, that decreases model sizes. Compared with FP16 — the default method that most models feature — FP4 uses less than half of the memory, and 50 Series GPUs provide over 2x performance compared with the previous generation. This can be done with virtually no loss in quality with advanced quantization methods offered by NVIDIA TensorRT Model Optimizer.

For example, Black Forest Labs’ FLUX.1 [dev] model at FP16 requires over 23GB of VRAM, meaning it can only be supported by the GeForce RTX 4090 and professional GPUs. With FP4, FLUX.1 [dev] requires less than 10GB, so it can run locally on more GeForce RTX GPUs.

With a GeForce RTX 4090 with FP16, the FLUX.1 [dev] model can generate images in 15 seconds with 30 steps. With a GeForce RTX 5090 with FP4, images can be generated in just over five seconds.

Get Started With the New AI APIs for PCs
NVIDIA NIM microservices and AI Blueprints are expected to be available starting next month, with initial hardware support for GeForce RTX 50 Series, GeForce RTX 4090 and 4080, and NVIDIA RTX 6000 and 5000 professional GPUs. Additional GPUs will be supported in the future.

NIM-ready RTX AI PCs are expected to be available from Acer, ASUS, Dell, GIGABYTE, HP, Lenovo, MSI, Razer and Samsung, and from local system builders Corsair, Falcon Northwest, LDLC, Maingear, Mifcon, Origin PC, PCS and Scan.

GeForce RTX 50 Series GPUs and laptops deliver game-changing performance, power transformative AI experiences, and enable creators to complete workflows in record time. Rewatch NVIDIA CEO Jensen Huang’s  keynote to learn more about NVIDIA’s AI news unveiled at CES.

See notice regarding software product information.

Categories: Generative AI
Tags: AI Decoded | Artificial Intelligence | GeForce | NVIDIA RTX
Load Comments
Subscribe Widget
All NVIDIA News

How This NVIDIA Kaggle Grandmaster Merges Innovation and Play


NVIDIA Releases NIM Microservices to Safeguard Applications for Agentic AI

Eternal Night Falls on GeForce NOW
Fantastic Four-ce Awakens: Season One of ‘Marvel Rivals’ Joins GeForce NOW


How AI Is Enhancing Surgical Safety and Education


NVIDIA GTC 2025: Quantum Day to Illuminate the Future of Quantum Computing

Post navigation
NVIDIA and Partners Launch Agentic AI Blueprints to Automate Work for Every Enterprise
Developers can now build and deploy custom AI agents that can reason, plan and take action with new NVIDIA AI Blueprints that include NVIDIA NIM microservices, NVIDIA NeMo and agentic AI frameworks from leading providers.
January 6, 2025 by Justin Boitano

 Share
  
New NVIDIA AI Blueprints for building agentic AI applications are poised to help enterprises everywhere automate work.

With the blueprints, developers can now build and deploy custom AI agents. These AI agents act like “knowledge robots” that can reason, plan and take action to quickly analyze large quantities of data, summarize and distill real-time insights from video, PDF and other images.

CrewAI, Daily, LangChain, LlamaIndex and Weights & Biases are among leading providers of agentic AI orchestration and management tools that have worked with NVIDIA to build blueprints that integrate the NVIDIA AI Enterprise software platform, including NVIDIA NIM microservices and NVIDIA NeMo, with their platforms. These five blueprints — comprising a new category of partner blueprints for agentic AI — provide the building blocks for developers to create the next wave of AI applications that will transform every industry.

In addition to the partner blueprints, NVIDIA is introducing its own new AI Blueprint for PDF to podcast, as well as another to build AI agents for video search and summarization. These are joined by four additional NVIDIA Omniverse Blueprints that make it easier for developers to build simulation-ready digital twins for physical AI.

To help enterprises rapidly take AI agents into production, Accenture is announcing AI Refinery for Industry built with NVIDIA AI Enterprise, including NVIDIA NeMo, NVIDIA NIM microservices and AI Blueprints.

The AI Refinery for Industry solutions — powered by Accenture AI Refinery with NVIDIA — can help enterprises rapidly launch agentic AI across fields like automotive, technology, manufacturing, consumer goods and more.

Agentic AI Orchestration Tools Conduct a Symphony of Agents
Agentic AI represents the next wave in the evolution of generative AI. It enables applications to move beyond simple chatbot interactions to tackle complex, multi-step problems through sophisticated reasoning and planning. As explained in NVIDIA founder and CEO Jensen Huang’s CES keynote, enterprise AI agents will become a centerpiece of AI factories that generate tokens to create unprecedented intelligence and productivity across industries.

Agentic AI orchestration is a sophisticated system designed to manage, monitor and coordinate multiple AI agents working together — key to developing reliable enterprise agentic AI systems. The agentic AI orchestration layer from NVIDIA partners provides the glue needed for AI agents to effectively work together.

The new partner blueprints, now available from agentic AI orchestration leaders, offer integrations with NVIDIA AI Enterprise software, including NIM microservices and NVIDIA NeMo Retriever, to boost retrieval accuracy and reduce latency of agent workflows. For example:

CrewAI is using new Llama 3.3 70B NVIDIA NIM microservices and the NVIDIA NeMo Retriever embedding NIM microservice for its blueprint for code documentation for software development. The blueprint helps ensure code repositories remain comprehensive and easy to navigate.
Daily’s voice agent blueprint, powered by the company’s open-source Pipecat framework, uses the NVIDIA Riva automatic speech recognition and text-to-speech NIM microservice, along with the Llama 3.3 70B NIM microservice to achieve real-time conversational AI.
LangChain is adding Llama 3.3 70B NVIDIA NIM microservices to its structured report generation blueprint. Built on LangGraph, the blueprint allows users to define a topic and specify an outline to guide an agent in searching the web for relevant information, so it can return a report in the requested format.
LlamaIndex’s document research assistant for blog creation blueprint harnesses NVIDIA NIM microservices and NeMo Retriever to help content creators produce high-quality blogs. It can tap into agentic-driven retrieval-augmented generation with NeMo Retriever to automatically research, outline and generate compelling content with source attribution.
Weights & Biases is adding its W&B Weave capability to the AI Blueprint for AI virtual assistants, which features the Llama 3.1 70B NIM microservice. The blueprint can streamline the process of debugging, evaluating, iterating and tracking production performance and collecting human feedback to support seamless integration and faster iterations for building and deploying agentic AI applications.
Summarize Many, Complex PDFs While Keeping Proprietary Data Secure 
With trillions of PDF files — from financial reports to technical research papers — generated every year, it’s a constant challenge to stay up to date with information.

NVIDIA’s PDF to podcast AI Blueprint provides a recipe developers can use to turn multiple long and complex PDFs into AI-generated readouts that can help professionals, students and researchers efficiently learn about virtually any topic and quickly understand key takeaways.

The blueprint — built on NIM microservices and text-to-speech models — allows developers to build applications that extract images, tables and text from PDFs, and convert the data into easily digestible audio content, all while keeping data secure.

For example, developers can build AI agents that can understand context, identify key points and generate a concise summary as a monologue or a conversation-style podcast, narrated in a natural voice. This offers users an engaging, time-efficient way to absorb information at their desired speed.

Test, Prototype and Run Agentic AI Blueprints in One Click
NVIDIA Blueprints empower the world’s more than 25 million software developers to easily integrate AI into their applications across various industries. These blueprints simplify the process of building and deploying agentic AI applications, making advanced AI integration more accessible than ever.

With just a single click, developers can now build and run the new agentic AI Blueprints as NVIDIA Launchables. These Launchables provide on-demand access to developer environments with predefined configurations, enabling quick workflow setup.

By containing all necessary components for development, Launchables support consistent and reproducible setups without the need for manual configuration or overhead — streamlining the entire development process, from prototyping to deployment.

Enterprises can also deploy blueprints into production with the NVIDIA AI Enterprise software platform on data center platforms including Dell Technologies, Hewlett Packard Enterprise, Lenovo and Supermicro, or run them on accelerated cloud platforms from Amazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure.

Accenture and NVIDIA Fast-Track Deployments With AI Refinery for Industry
Accenture is introducing its new AI Refinery for Industry with 12 new industry agent solutions built with NVIDIA AI Enterprise software and available from the Accenture NVIDIA Business Group. These industry-specific agent solutions include revenue growth management for consumer goods and services, clinical trial companion for life sciences, industrial asset troubleshooting and B2B marketing, among others.

AI Refinery for Industry offerings include preconfigured components, best practices and foundational elements designed to fast-track the development of AI agents. They provide organizations the tools to build specialized AI networks tailored to their industry needs.

Accenture plans to launch over 100 AI Refinery for Industry agent solutions by the end of the year.

Get started with AI Blueprints and join NVIDIA at CES.

See notice regarding software product information.

Categories: Generative AI
Tags: Artificial Intelligence | CES 2025 | NVIDIA AI Enterprise | NVIDIA Blueprints | NVIDIA NeMo | NVIDIA NIM | Physical AI | Riva
Load Comments
Subscribe Widget
All NVIDIA News

How This NVIDIA Kaggle Grandmaster Merges Innovation and Play


NVIDIA Releases NIM Microservices to Safeguard Applications for Agentic AI

Eternal Night Falls on GeForce NOW
Fantastic Four-ce Awakens: Season One of ‘Marvel Rivals’ Joins GeForce NOW


How AI Is Enhancing Surgical Safety and Education


NVIDIA GTC 2025: Quantum Day to Illuminate the Future of Quantum Computing

Post navigation
Corporate Information
About NVIDIA
Corporate Overview
Technologies
NVIDIA Research
Investors
Social Responsibility
NVIDIA Foundation
Get Involved
Forums
Careers
Developer Home
Join the Developer Program
NVIDIA Partner Network
NVIDIA Inception
Resources for Venture Capitalists
Venture Capital (NVentures)
Technical Training
Training for IT Professionals
Professional Services for Data Science
News & Events
Newsroom
NVIDIA Blog
NVIDIA Technical Blog
Webinars
Stay Informed
Events Calendar
NVIDIA GTC
NVIDIA On-Demand
Explore our regional blogs and other social networks
Privacy Policy Manage My Privacy Legal Accessibility Product Security Contact
Copyright © 2025 NVIDIA Corporation
USA - United States

Feedback